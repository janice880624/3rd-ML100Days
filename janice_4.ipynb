{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "janice_4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janice880624/3rd-ML100Days/blob/master/janice_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydaFyvSicFwt",
        "colab_type": "text"
      },
      "source": [
        "####1. 詞性標註\n",
        "詞彙按它們的詞性(parts-of-speech,POS)分類以及相應的標註它們的過程, 詞性包括:名詞、動詞、形容詞, 副詞等.\n",
        "\n",
        "####2. 中文字元的顯示\n",
        "Python內部編碼是unicode, 所以輸出中文常常像這樣\"/u4eba/u5de5\", 用print函式輸出時, 將自動轉換成本地字符集, 也可以使用encode(‘utf-8’)函式轉換.\n",
        "\n",
        "####3. 資料集,訓練集,評估\n",
        "有監督的機器學習一般都是把資料分成兩個部分, 一部分用於訓練, 一部分用於測試, 還可以通過不同分組交叉驗證. Nltk提供了evaluate()函式評估標註效果.\n",
        "\n",
        "####4. 預設標註(Default Tagger)\n",
        "事先對語料庫做了統計(利用nltk.FreqDist()), 出現最多的是名詞.\n",
        "在這裡,預設標註為名詞\n",
        "\n",
        "####5. 正則表示式標註(Regexp Tagger)\n",
        "用匹配模式分配標記給識別符號.在英文處理中,常用此方式識別各種形態(時態,字尾等),中文識別中也可以使用它來識別標點,數字等.\n",
        "\n",
        "####6. 一元標註(Unigram Tagger)\n",
        "一元標註基於一個簡單的統計演算法: 對每個識別符號分配這個獨特的識別符號最有可能的標記.\n",
        "在這裡就是分配給具體單詞,它最常出現的詞性.\n",
        "\n",
        "####7. 多元標註(N-gram Tagger)\n",
        "多元標註使用訓練集來確定對每個上下文哪個詞性標記最有可能。上下文指當前詞和它前面 n-1 個識別符號的詞性標記.\n",
        "在這裡,就是找一些規律, 比如: XX常出現在名詞之前, YY常出現在動詞之後. 通過某個詞以及它之前那個詞的詞性來判斷它的詞性. 這就是二元標註. 同理,可以生成三元甚至多元標註.詞的距離越遠影響越小, 也更佔用資源, 一般二元到三元就夠了.\n",
        "\n",
        "####8. 組合標註\n",
        "更精確的演算法在很多時候落後於具有更廣覆蓋範圍的演算法(比如滿足三元標的詞可能非常少), 所以有時我們組合多個標註器,\n",
        "在這裡,組合 bigram 標註器、unigram 標註器和一個預設標註器"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3ikjcsA_bPG",
        "colab_type": "code",
        "outputId": "191bb9e3-7b3c-4dfa-c1c3-7854cf34eea1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# NLTK 全名是 Natural Language Tool Kit， 是一套基於 Python 的自然語言處理工具箱。\n",
        "import nltk\n",
        "\n",
        "# 在使用NLTK執行分詞之前，我們需要先安裝「punkt」部件。「punkt」包含了許多預訓練好的分詞模型。如果沒有安裝「punkt」，我們在使用時系統將會報錯，提示我們進行安裝。\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBSSrq4hAUUC",
        "colab_type": "code",
        "outputId": "68af25c0-9d13-4097-e687-f0548ec7d40c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# 使用 tokenize 進行斷詞\n",
        "# 使用 word_tokenize 可以進行斷詞，而 sent_tokenize 可以幫助我們斷句\n",
        "words = nltk.word_tokenize('I am a fish')\n",
        "print(words)\n",
        "\n",
        "# 詞性標註\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "word_tag = nltk.pos_tag(words)\n",
        "print(word_tag)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', 'am', 'a', 'fish']\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[('I', 'PRP'), ('am', 'VBP'), ('a', 'DT'), ('fish', 'NN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6ODF7cTDAkV",
        "colab_type": "code",
        "outputId": "4f1952fb-edc6-488f-c3be-6a36199541aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# 下載這個文本\n",
        "# sinica_treebank是繁體中文的語料庫，用法和corpus內其他文章滿相似的\n",
        "nltk.download('sinica_treebank')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package sinica_treebank to /root/nltk_data...\n",
            "[nltk_data]   Package sinica_treebank is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cT__djzjRW50",
        "colab_type": "code",
        "outputId": "42487e42-8d7f-4b28-ed5d-06843e0ee6ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# 中文語料庫sinica_treebank，該庫使用繁體中文，該庫也被標註了詞性\n",
        "sinica_treebank = nltk.corpus.sinica_treebank\n",
        "\n",
        "# 顯示單詞列表\n",
        "words = sinica_treebank.words('parsed')\n",
        "print(words[:40])\n",
        "\n",
        "# 顯示標註好詞性的單詞列表\n",
        "words_tag = sinica_treebank.tagged_words('parsed')\n",
        "print(words_tag[:40])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['一', '友情', '嘉珍', '和', '我', '住在', '同一條', '巷子', '我們', '是', '鄰居', '也', '是', '同班', '同學', '我們', '常常', '一起', '上學', '一起', '回家', '有一天', '上學', '時', '我', '到', '她', '家', '等候', '按', '了', '門鈴', '卻', '沒有', '任何', '動靜', '正當', '我', '想', '離開']\n",
            "[('一', 'Neu'), ('友情', 'Nad'), ('嘉珍', 'Nba'), ('和', 'Caa'), ('我', 'Nhaa'), ('住在', 'VC1'), ('同一條', 'DM'), ('巷子', 'Nab'), ('我們', 'Nhaa'), ('是', 'V_11'), ('鄰居', 'Nab'), ('也', 'Dbb'), ('是', 'V_11'), ('同班', 'Nv3'), ('同學', 'Nab'), ('我們', 'Nhaa'), ('常常', 'Dd'), ('一起', 'Dh'), ('上學', 'VA4'), ('一起', 'Dh'), ('回家', 'VA13'), ('有一天', 'DM'), ('上學', 'VA4'), ('時', 'Ng'), ('我', 'Nhaa'), ('到', 'P61'), ('她', 'Nhaa'), ('家', 'Ncb'), ('等候', 'VK2'), ('按', 'VC2'), ('了', 'Di'), ('門鈴', 'Nab'), ('卻', 'Dbb'), ('沒有', 'VJ3'), ('任何', 'Neqa'), ('動靜', 'Nad'), ('正當', 'P16'), ('我', 'Nhaa'), ('想', 'VE2'), ('離開', 'VC2')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqD1KWtSS987",
        "colab_type": "text"
      },
      "source": [
        "- CC 連接詞 and, or,but, if, while,although\n",
        "- CD 數量詞 twenty-four, fourth, 1991,14:24\n",
        "- DT 限定词 the, a, some, most,every, no\n",
        "- EX 存在量词 there, there's\n",
        "- FW 外来词 dolce, ersatz, esprit, quo,maitre\n",
        "- IN 介係詞 on, of,at, with,by,into, under\n",
        "- JJ 形容词 new,good, high, special, big, local\n",
        "- JJR 比較級 bleaker braver breezier briefer brighter brisker\n",
        "- JJS 最高級 calmest cheapest choicest classiest cleanest clearest\n",
        "- LS 標記 A A. B B. C C. D E F First G H I J K\n",
        "- MD 情緒動詞 can cannot could couldn't\n",
        "- NN 名詞 year,home, costs, time, education\n",
        "- NNS 名詞復數 undergraduates scotches\n",
        "- NNP 專有名詞 Alison,Africa,April,Washington\n",
        "- NNPS 專有名詞複數 Americans Americas Amharas Amityvilles\n",
        "- PDT 前限定词 all both half many\n",
        "- POS 所有格 ' 's\n",
        "- PRP 人稱代名詞 hers herself him himself hisself\n",
        "- PRP 所有格 her his mine my our ours\n",
        "- RB 副詞 occasionally unabatingly maddeningly\n",
        "- RBR 副詞比較級 further gloomier grander\n",
        "- RBS 副詞最高級 best biggest bluntest earliest\n",
        "- RP 虛詞 aboard about across along apart\n",
        "- SYM 符号 % & ' '' ''. ) )\n",
        "- TO 詞 to to\n",
        "- UH 感嘆詞 Goodbye Goody Gosh Wow\n",
        "- VB 動詞 ask assemble assess\n",
        "- VBD 動詞過去式 dipped pleaded swiped\n",
        "- VBG 動詞現在分詞 telegraphing stirring focusing\n",
        "- VBN 動詞過去分詞 multihulled dilapidated aerosolized\n",
        "- VBP 動詞現在式非第三人稱時態 predominate wrap resort sue\n",
        "- VBZ 動詞現在式第三人稱時態 bases reconstructs marks\n",
        "- WDT Wh 限定詞 who,which,when,what,where,how\n",
        "- WP WH代詞 that what whatever\n",
        "- WP WH代詞所有格 whose\n",
        "- WRB WH複詞\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CM3t65ZDRY5r",
        "colab_type": "code",
        "outputId": "e25201c0-d22b-44c0-edf5-9fe3e9c94fbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 顯示標註好詞性的句子列表\n",
        "sents = sinica_treebank.tagged_sents('parsed')\n",
        "print(sents[3])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('我們', 'Nhaa'), ('是', 'V_11'), ('鄰居', 'Nab')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjFtheygUkwJ",
        "colab_type": "code",
        "outputId": "6ecd52ba-6215-4454-928c-6c5b259d94c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# 將資料分成訓練集和測試集\n",
        "# 90％為測試數據，其餘10％為測試數據\n",
        "tagged_sents = nltk.corpus.sinica_treebank.tagged_sents()\n",
        "size = int(len(tagged_sents) * 0.9)  # 0.9關係到分割比例\n",
        "train_sents = tagged_sents[:size]\t\n",
        "test_sents = tagged_sents[size:]\t\n",
        "\n",
        "# 組合標註器\n",
        "# 解決精度和覆蓋範圍之間權衡的一個辦法是儘可能地使用更精確的演算法，但卻在很多時候卻遜於覆蓋範圍更廣的演算法．如組合bigram標註器和unigram標註器和一個預設標註器．\n",
        "# 嘗試使用bigram標註器標註識別符號\n",
        "# 如果bigram標準器無法找到標記，嘗試unigram標註器\n",
        "# 如果unigram標註器也無法找到標記，使用預設標註器\n",
        "\n",
        "# 預設標註器 \n",
        "t0 = nltk.DefaultTagger('NN')\n",
        "print (t0.evaluate(test_sents))  #評估\n",
        "\n",
        "# 一元標註器(Unigram Tagging)\n",
        "t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
        "print (t1.evaluate(test_sents))  #評估\n",
        "\n",
        "# 一般的N-gram的標註\n",
        "t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
        "print (t2.evaluate(test_sents))\t  #評估"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n",
            "0.6942689362967368\n",
            "0.6995465695383929\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgR3JWfnUzDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 載入套件\n",
        "from keras.layers.core import Activation, Dense,Dropout\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing import sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "import collections\n",
        "import numpy as np\n",
        "from keras.models import load_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er86PwhVf9ha",
        "colab_type": "code",
        "outputId": "99142ea4-41d7-4744-edf8-07da92b53c18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "## 探索數據分析(EDA)\n",
        "# 在開始前，先對所用數據做個初步探索。特別地，我們需要知道數據中有多少個不同的單詞，每句話由多少個單詞組成。\n",
        "\n",
        "max_len = 0  #初始設定最長的長度為0\n",
        "\n",
        "# 詞頻\n",
        "word_freqs = collections.Counter()\n",
        "\n",
        "# 樣本數\n",
        "num_recs = 0\n",
        "\n",
        "with open('Sentiment1_training.txt','r', encoding='UTF-8') as f: #開檔案\n",
        "    for line in f:\n",
        "        label, sentence = line.strip().split(\"\\t\") # 分割\n",
        "        # raw_input()  -> ' insert 0 5     '\n",
        "        # raw_input().strip() -> 'insert 0 5'\n",
        "        # raw_input().strip().strip() -> ['insert', '0', '5']\n",
        "        words = nltk.word_tokenize(sentence.lower())  # 使用 tokenize 進行斷詞並將大寫換小寫\n",
        "        w_len = len(words)   # 利用w_len儲存w的長度\n",
        "        if w_len > max_len:  # 判斷w這個字的長度是否大於目前的字\n",
        "            max_len = w_len  # 如果w這個字的長度大於目前的字，則將原本的長度變成目前長度最長字的長度\n",
        "        for word in words:  # 判斷 word 是否在 words 裡面\n",
        "            word_freqs[word] += 1  # 詞頻數+1\n",
        "        num_recs += 1 # 樣本數+1\n",
        "\n",
        "print('max_len ',max_len)\n",
        "print('nb_words ', len(word_freqs))\n",
        "\n",
        "# 可見一共有 2328 個不同的單詞，包括標點符號。\n",
        "# 每句話最多包含 42 個單詞。\n",
        "\n",
        "# 根據不同單詞的個數 (nb_words)，我們可以把詞彙表的大小設為一個定值，並且對於不在詞彙表里的單詞，把它們用偽單詞 UNK 代替。\n",
        "# 根據句子的最大長度 (max_lens)，我們可以統一句子的長度，把短句用 0 填充。\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max_len  42\n",
            "nb_words  2328\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boS0AXa4hnYS",
        "colab_type": "code",
        "outputId": "00a4c5d8-34b4-42b0-95cf-6ffbba0c1b64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# 準備數據 先找出所有的單字，給予序號，再將每一句訓練資料的單字轉為序號。\n",
        "\n",
        "# 最大長度\n",
        "max_features = 2000 \n",
        "\n",
        "# 單一句子中最大的單字數\n",
        "max_sentence_length = 40\n",
        "\n",
        "# 依前所述，我們把 VOCABULARY_SIZE 設為 2002。包含訓練數據中按詞頻從大到小排序後的前 2000 個單詞，外加一個偽單詞 UNK 和填充單詞 0\n",
        "vocab_size = min(max_features, len(word_freqs)) + 2\n",
        "\n",
        "# 建立兩個表，word2index和 index2word，用於單詞和數字轉換，把句子轉換成數字序列，長度統一到 max_sentence_length，不夠的填0，多出的截掉\n",
        "# word_index長度等於所有已知單字\n",
        "word_index = {x[0]: i+2 for i, x in enumerate(word_freqs.most_common(max_features))}\n",
        "word_index[\"PAD\"] = 0\n",
        "word_index[\"UNK\"] = 1\n",
        "\n",
        "# 接下來建立兩個 lookup tables，分別是 word2index 和 index2word，用於單詞和數字轉換。\n",
        "index2word = {v:k for k, v in word_index.items()}\n",
        "X = np.empty(num_recs,dtype=list)\n",
        "y = np.zeros(num_recs)\n",
        "i=0\n",
        "\n",
        "print(X)\n",
        "print(y)\n",
        "print(i)\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[None None None ... None None None]\n",
            "[0. 0. 0. ... 0. 0. 0.]\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3VeRxRorYE1",
        "colab_type": "code",
        "outputId": "b3f393a4-9e91-4362-b77a-2411a279d2ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# 讀取訓練資料，將每一單字以 dictionary 儲存\n",
        "with open('Sentiment1_training.txt','r', encoding='UTF-8') as f:\n",
        "    for line in f:\n",
        "        label, sentence = line.strip().split(\"\\t\")  # 分割\n",
        "        words = nltk.word_tokenize(sentence.lower())  # 使用 tokenize 進行斷詞並將大寫換小寫\n",
        "        seqs = [] #　建立空陣列\n",
        "        for word in words:   # 判斷 word 是否在 words 裡面\n",
        "            if word in word_index: # 判斷 word 是否在　word_index裡面　\n",
        "                seqs.append(word_index[word])# 如果是的話增加word\n",
        "            else:\n",
        "                seqs.append(word_index[\"UNK\"]) #否則的話增加　\"UNK\"\n",
        "        X[i] = seqs\n",
        "        y[i] = int(label)\n",
        "        i += 1#i+1\n",
        "        \n",
        "        \n",
        "print(X)\n",
        "print(y)\n",
        "print(i)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[list([5, 10, 9, 12, 101, 17, 48, 22, 4])\n",
            " list([67, 19, 5, 115, 969, 970, 2, 358, 136, 110, 3, 44, 317, 319, 23, 971, 3, 6, 10, 9, 12, 137, 118, 972, 341, 67, 4])\n",
            " list([2, 122, 5, 10, 9, 12, 18, 325, 4]) ...\n",
            " list([34, 2, 303, 96, 3, 156, 5, 304, 26, 220, 3, 2, 58, 305, 38, 73, 37, 2, 306, 5, 26, 11, 13, 4])\n",
            " list([94, 11, 13, 17, 144, 18, 127, 26, 4])\n",
            " list([89, 3, 6, 11, 13, 19, 18, 87, 26, 4])]\n",
            "[1. 1. 1. ... 0. 0. 0.]\n",
            "7086\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQRJorPltiuS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 字句長度不足補空白        \n",
        "X = sequence.pad_sequences(X, maxlen = max_sentence_length)\n",
        "\n",
        "# 最後是劃分數據，80% 作為訓練數據，20% 作為測試數據。\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkHGlYc8xIPC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "03430d81-23ba-44b8-c332-96065792e99b"
      },
      "source": [
        "# 模型構建\n",
        "embedding_size = 128\n",
        "hidden_layer_size = 64\n",
        "batch_size = 32\n",
        "num_epochs = 20\n",
        "model = Sequential()\n",
        "\n",
        "# 加『嵌入』層\n",
        "# 將『數字list』轉為32維度的向量\n",
        "# 建立2002維度的向量  ->  因為vocab_size為2002\n",
        "# list每筆為max_sentence_length個字 \n",
        "# 加入Dropout避免overfitting   ->   在每一次訓練迭代時隨機放棄20%神經元\n",
        "model.add(Embedding(vocab_size, embedding_size, input_length=max_sentence_length))\n",
        "model.add(Dropout(0.2))  \n",
        "\n",
        "# 加『LSTM』層\n",
        "model.add(LSTM(hidden_layer_size, dropout=0.2, recurrent_dropout=0.2))\n",
        "\n",
        "#隱藏層有256個神經元\n",
        "#定義激活函數  ->  relu\n",
        "#加入Dropout避免overfitting   ->   在每一次訓練迭代時隨機放棄35%神經元\n",
        "model.add(Dense(units=256, activation='relu' ))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "#輸出層有1個神經元\n",
        "#定義激活函數  ->  sigmoid\n",
        "model.add(Dense(units=1,activation='sigmoid' ))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 40, 128)           256256    \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 64)                49408     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 256)               16640     \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 322,561\n",
            "Trainable params: 322,561\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OYgHyGzytnS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# binary_crossentropy:二分法\n",
        "# 設定損失函數\n",
        "# 設定最優化的訓練方法\n",
        "# 設定評估模型方式\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",metrics=[\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycMGDD4Dyv_L",
        "colab_type": "code",
        "outputId": "bc62eb26-a29d-4034-dc76-a89f72967144",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 733
        }
      },
      "source": [
        "# 模型訓練\n",
        "# 設定訓練參數  ->  x:feature  y:label\n",
        "# 設定每一次訓練幾筆資料\n",
        "# 設定執行幾次週期\n",
        "# 顯示訓練過程\n",
        "# 設定訓練與驗證資料比例\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs,validation_data=(x_test, y_test))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 5668 samples, validate on 1418 samples\n",
            "Epoch 1/20\n",
            "5668/5668 [==============================] - 12s 2ms/step - loss: 0.1944 - acc: 0.9143 - val_loss: 0.0629 - val_acc: 0.9788\n",
            "Epoch 2/20\n",
            "5668/5668 [==============================] - 10s 2ms/step - loss: 0.0147 - acc: 0.9956 - val_loss: 0.0491 - val_acc: 0.9838\n",
            "Epoch 3/20\n",
            "5668/5668 [==============================] - 10s 2ms/step - loss: 0.0074 - acc: 0.9977 - val_loss: 0.0566 - val_acc: 0.9845\n",
            "Epoch 4/20\n",
            "5668/5668 [==============================] - 10s 2ms/step - loss: 0.0038 - acc: 0.9989 - val_loss: 0.0697 - val_acc: 0.9803\n",
            "Epoch 5/20\n",
            "5668/5668 [==============================] - 10s 2ms/step - loss: 0.0042 - acc: 0.9991 - val_loss: 0.0671 - val_acc: 0.9866\n",
            "Epoch 6/20\n",
            "5668/5668 [==============================] - 10s 2ms/step - loss: 7.1163e-04 - acc: 0.9998 - val_loss: 0.0906 - val_acc: 0.9866\n",
            "Epoch 7/20\n",
            "5668/5668 [==============================] - 10s 2ms/step - loss: 0.0044 - acc: 0.9995 - val_loss: 0.0754 - val_acc: 0.9831\n",
            "Epoch 8/20\n",
            "5668/5668 [==============================] - 10s 2ms/step - loss: 0.0067 - acc: 0.9993 - val_loss: 0.0821 - val_acc: 0.9873\n",
            "Epoch 9/20\n",
            "5668/5668 [==============================] - 10s 2ms/step - loss: 0.0031 - acc: 0.9996 - val_loss: 0.0815 - val_acc: 0.9803\n",
            "Epoch 10/20\n",
            "5668/5668 [==============================] - 10s 2ms/step - loss: 0.0011 - acc: 0.9998 - val_loss: 0.0901 - val_acc: 0.9810\n",
            "Epoch 11/20\n",
            "5668/5668 [==============================] - 10s 2ms/step - loss: 0.0012 - acc: 0.9998 - val_loss: 0.0821 - val_acc: 0.9873\n",
            "Epoch 12/20\n",
            "5668/5668 [==============================] - 10s 2ms/step - loss: 7.9114e-04 - acc: 0.9998 - val_loss: 0.0931 - val_acc: 0.9838\n",
            "Epoch 13/20\n",
            "5668/5668 [==============================] - 10s 2ms/step - loss: 7.4446e-04 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9852\n",
            "Epoch 14/20\n",
            "5668/5668 [==============================] - 10s 2ms/step - loss: 3.8728e-04 - acc: 0.9998 - val_loss: 0.0892 - val_acc: 0.9859\n",
            "Epoch 15/20\n",
            "5668/5668 [==============================] - 10s 2ms/step - loss: 2.6778e-04 - acc: 0.9998 - val_loss: 0.0921 - val_acc: 0.9859\n",
            "Epoch 16/20\n",
            "5668/5668 [==============================] - 10s 2ms/step - loss: 2.6991e-04 - acc: 0.9998 - val_loss: 0.0922 - val_acc: 0.9866\n",
            "Epoch 17/20\n",
            "5668/5668 [==============================] - 10s 2ms/step - loss: 2.8633e-04 - acc: 0.9998 - val_loss: 0.0848 - val_acc: 0.9880\n",
            "Epoch 18/20\n",
            "5668/5668 [==============================] - 10s 2ms/step - loss: 2.6583e-04 - acc: 0.9998 - val_loss: 0.0848 - val_acc: 0.9894\n",
            "Epoch 19/20\n",
            "5668/5668 [==============================] - 10s 2ms/step - loss: 2.5279e-04 - acc: 1.0000 - val_loss: 0.0895 - val_acc: 0.9880\n",
            "Epoch 20/20\n",
            "5668/5668 [==============================] - 10s 2ms/step - loss: 2.7721e-04 - acc: 0.9998 - val_loss: 0.0981 - val_acc: 0.9859\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efbe4e0b940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bm6zEQFryxV2",
        "colab_type": "code",
        "outputId": "b0caaf92-922d-4d85-a7ce-fafb2bcf8ae6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# 使用model.evaluate進行評估模型準確率，評估後的準確率會儲存在sorces\n",
        "# 設定測試資料參數  ->  x:features  y:label\n",
        "sorces =  model.evaluate ( x_test , y_test , verbose=1 )  \n",
        "sorces[1]"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1418/1418 [==============================] - 0s 284us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9858956276445698"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhBk8W2T-YxR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}